= 카프카 핵심 가이드 

== 1. 카프카 시작하기

* publish/subscribe message
** 전송자(발행하는 쪽)가 데이터(메시지)를 보낼 때 직접 수신자(구독하는 쪽)로 보내지 않는다.
** 전송자를 어떤 형태로든 메시지를 분류해서 보내고, 수신자는 분류된 메시지를 구독한다.
** 대개 발행된 메시지를 전달 받고 중계해주는 중간 지점 역할을 하는 브로커~broker~가 있다.
* 아파치 카프카는 메시지 발행/구독 시스템이다.
** '분산 커밋 로그' or '분산 스트리밍 플랫폼'이라고 불리기도 한다.
** 파일시스템이나 데이터베이스 커밋 로그~commit{sp}log~는 모든 트랜잭션 기록을 지속성~durable~있게 보존함으로써 시스템의 상태를 일관성~consistency~ 있게 복구할 수 있도록 고안되었다.
** 카프카에 저장된 데이터는 순서를 유지한채로 지속성 있게 보관되면 결정적~deterministic~으로 읽을 수 있다.
** 확장시 실패하더라도 데이터 사용에는 문제가 없도록 시스템 안에서 데이터를 분산시켜 저장할 수 있다.
* 일정 기간 동안 메시지를 지속성~durability~ 있게 보관하는 보존~retention~ 기능이 있다.
** 특정 기간동안 메시지를 보전하거나, 파티션의 크기가 특정 사이즈에 도달할 떄까지 데이터를 보존한다.

=== 메시지~message~

* 카프카에서 데이터의 기본 단위. 데이터베이스의 row나 record와 비슷해보일 수 있다. 카프카 입장에서 메시지는 단순히 바이트 배열일 뿐이므로 특정한 형식이나 의미가 없다.
* 메시지는 key라 불리는 메타데이터를 포함할 수 있다. key는 메시지를 저장할 파티션을 결정하기 위해 사용된다.
* 특정한 토픽에 쓰여진다.

=== 배치~batch~

* 카프카는 효율성을 위해 메시지를 배치 단위로 저장한다. 배치는 토픽의 파티션에 쓰여지는 메시지들의 집합이다.
* 메시지를 배치 단위로 쓰면 네트워크상 오버헤드를 줄일 수 있다. 허나 지연~latency~과 처리량~throughput~ 사이에 트레이드오프를 발생시킨다.

=== 스키마

* 카프카 입장에서 메시지는 단순한 바이트 배열일 뿐이지만, 내용일 이해하기 쉽도록 일정한 구조(혹은 스키마)를 부여하는 것이 권장된다.
* JSON, XML, Avro(카프카 개발자들이 선호)이 있다.

=== 토픽~topic~

* 카프카에 저장되는 메시지는 토픽 단위로 분류된다. 토픽은 다시 여러 개의 파티션~partition~으로 나눠진다.
* 파티션에 메시지가 쓰일 땐 append-only 형태로 쓰여진다. 읽을 땐 FIFO로 읽힌다.
* 토픽에 여러 파티션이 있는 만큼 토픽 안의 메시지 전체에 대해 순서는 보장되지 않으며, 단일 파티션 안에서만 순서가 보장될 뿐이다.
* 파티션은 복제될 수 있다. 서로 다른 서버들이 동일한 파티션의 복제본을 저장하고 있기 때문에 서버 중 하나에 장애가 발생해도 문제 없다.
* 스트림~stream~은 (파티션의 갯수와 상관없이) 하나의 토픽에 저장되는 데이터로 간주되며, 프로듀서~producer~로부터 컨슈머~consumer~로의 하나의 데이터 흐름을 나타낸다.
** 메시지 집합을 스크림이라는 용어로 부르는 것은 ...
** 이러한 방식의 처리는 데이터를 시간이 흐른 뒤 한꺼번에 대량으로 처리하는 하둡과 같은 오프라인 프레임워크와 대비된다.
* 토픽에는 로그 압착~log{sp}compaction~ 기능을 설정할 수 있는데,같이 키를 갖는 메시지 중 가장 최신의 것만 보존된다.

=== 프로듀서

* 메시지 생성한다.
* 발행자~publisher~ 혹은 작성자~writer~라고도 부른다.
* 프로듀서가 특정한 파티션을 지정해서 메시지를 쓸 수 있다. 메시지 키와 키값의 해시를 특정 파티션으로 대응시켜주는 파티셔너~partitioner~를 사용해서 구현한다.

=== 컨슈머

* 메시지를 읽는다.
* 구독자~subscriber~ 혹은 독자~reader~라고도 부른다.
* 토픽을 구독해서 여기에 저장된 메시지들을 각 파티션에 쓰여진 순서대로 읽어 온다.
* 컨슈머는 메시지의 오프셋~offset~을 기록함으로써 어느 메시지까지 읽었는지를 유지한다.
* 컨슈머는 컨슈머 그룹의 일원~consumer{sp}group~으로 작동한다.

=== 브로커

* 하나의 카프카 서버를 브로커라고 부른다.
* 브로커는 프로듀서로부터 메시지를 전달받아 오프셋을 할당한 뒤 디스크 저장소에 쓴다.
* 브로커는 컨슈머의 파티션 읽기~fetch~ 요청 역시 처리하고 발행된 메시지를 보내준다.

=== 클러스터

* 카프카 브로커는 클러스터의 일부로서 작동하도록 설계되었다.
** 하나의 클러스터 안에 여러 개의 브로커가 포함될 수 있다.
** 그중 하나의 브로커가 클러스터 컨트롤러 역할을 하게 된다.
*** 컨트롤러는 파티션을 브로커에 할당해주거나 장애나 발생한 브로커를 모니터링하는 등의 관리 기능을 담당한다.
* 파티션은 클러스터 안의 브로커 중 하나가 담당하며, 그 브로커는 파티션 리더~partition{sp}leader~라고 부른다.
* 복제된 파티션이 여러 브로커에 할당될 수도 있는데 이것들은 파티션의 팔로워~follewer~라고 부른다.
** 복제replication~~ 기능은 파티션의 메시지를 중복 저장함으로써 리더 브로커에 장애가 발생했을 때 팔로워 중 하나가 리더 역할을 이어받을 수 있도록 한다.

== 2. 카프카 설치하기

=== 주키퍼

* 아파치 카프카는 카프카 클러스터의 메타데이터와 컨슈터 클라이언트에 대한 정보를 저장하기 위해 아파키 주키퍼를 사용한다.
** 클러스터 환경 관리를 위한 (분산 시스템) 코디네이터
* 주키퍼는 고가용성을 보장하기 위해 앙상블~ensemble~이라 불리는 클러스터 단위로 작동하고록 설계되었다.
* 주키퍼가 사용하느 부하 분산 알고리즘 때문에 앙상블은 홀수 개의 서버를 가지는 것이 권장된다.
** 주키퍼가 요청에 응답하려면 앙상블 멤버(쿼럼~quorum~)의 과반 이상이 작동하고 있어야 하기 때문이다.

[quote]
____
Actually, the problem is not with ZooKeeper itself but with the concept of external metadata management.
(사실 문제는 ZooKeeper 자체가 아니라 외부 메타데이터 관리 개념에 있습니다.)
____

* KRaft
** 새로운 메타데이터 관리를 위함
** Raft 합의 프로토콜의 이벤트 기반 변형을 사용하는 kafka의 새로원 쿼럼 컨트롤러 서비스
*** https://seongjin.me/raft-consensus-algorithm/
*** https://zetawiki.com/wiki/Raft_%EC%95%8C%EA%B3%A0%EB%A6%AC%EC%A6%98

=== Quichstart

* https://kafka.apache.org/quickstart[Apache Kafka Quickstart]

[source, bash]
----
# step 1 - 다운로드
# https://kafka.apache.org/downloads
$ curl -O https://downloads.apache.org/kafka/3.5.1/kafka_2.13-3.5.1.tgz
$ tar -xzf kafka_2.13-3.5.1.tgz
$ cd kafka_2.13-3.5.1

# step 2 - 카프카 실행
# Kafka with KRaft
$ KAFKA_CLUSTER_ID="$(bin/kafka-storage.sh random-uuid)"
$ bin/kafka-storage.sh format -t $KAFKA_CLUSTER_ID -c config/kraft/server.properties
$ bin/kafka-server-start.sh config/kraft/server.properties

# step 3 - 토픽 생성/확인
$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --create --replication-factor 1 --partitions 1 --topic test
$ bin/kafka-topics.sh --bootstrap-server localhost:9092 --describe --topic test

# step 4 - 메시지 생성
$ bin/kafka-console-producer.sh --bootstrap-server localhost:9092 --topic test

# step 5 - 메시지 읽기
$ bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test --from-beginning

# kafka-ui - https://github.com/provectus/kafka-ui
# https://docs.kafka-ui.provectus.io/development/building/without-docker
# https://github.com/provectus/kafka-ui/releases
$ java -Dspring.config.additional-location=application.yml -jar kafka-ui-api-v0.7.1.jar
----

=== 브로커 설정

* `broker.id`
** 모든 카프카 브로커는 정숫값 식별자를 갖는다.
** 기본값은 0
* `listeners`
** 쉼표로 구분된 리스너 이름과 URI 목록
** 1024 미만의 포트 번호를 사용할 경우 루트 권한으로 카프카를 실행시켜야 하며, 이는 바람직하지 않다.
* `zookeeper.connect`
** 브로커의 메타데이터가 저장되는 주키퍼의 위치
* `log.dirs`
** 카프카는 모든 메시지를 로그 세그먼트~log{sp}segment~ 단위로 묶어서 지정된 디스크에 저장한다.
* `num.recovery.threads.per.data.dir`
** 카프카는 설정 가능한 스레드 풀을 사용해서 로그 세그먼트를 관리한다.
** 기본적으로 로그 디렉토리에 대해 하나의 스레드만이 사용된다.
** 이 설정에 따라 언클린 셧다운~unclean{sp}shutdown~ 이후 복구를 위한 재시작 시간이 차이날 수 있다.
* `auto.create.topics.enable`
** 브로커가 토픽을 자동으로 생성하도록 하는 상황
*** 프로듀서가 토픽에 메시지를 쓰기 시작할 때
*** 컨슈머가 토틱으로부터 메시지를 읽기 시작할 떄
*** 클라이언트가 토픽에 대한 메타데이터를 요청할 떄
** 이런 자동 생성을 제어하는 설정
* `auto.leader.rebalendce.enable`
** 모든 토픽의 리더 역할이 하나의 브로커에 집중됨으로써 카프카 클러스터의 균형이 때지는 수가 있음
** 이를 균등하게 분산되도록하는 설정. 파티션의 분포 상태를 주기적으로 확인하느 백그라운드 스레드가 시작됨
* `delete.topic.enable`
** 토픽을 임의로 삭제 못하게끔 하는 설정

== 3. 카프카 프로듀서
